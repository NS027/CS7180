{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Crash Course for Beginners\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is deep learning\n",
    "### Classification\n",
    "- **AI**\n",
    "- **Machine Learning:**\n",
    "Teaching computer to Recognize Patterns and Data\n",
    "- **Deep Learning:**<br>\n",
    "Technique that learns features and tasks directly from data.\n",
    "    - neural networks: hidden layers\n",
    "\n",
    "### why now\n",
    "- Data is prevalent\n",
    "- Improved hardware architectures\n",
    "- Tensorflow/Pytorch\n",
    "\n",
    "### Neural Networks\n",
    "- Input layer -> hidden layer -> output layer\n",
    "- neurons\n",
    "\n",
    "### Learning Process\n",
    "- Forward Propagation: *creation function*\n",
    "  - *Weight*: how important is the neuron\n",
    "  - *Bias*: allows for the shifting of the $\\sigma$ to the right or left\n",
    "$$\n",
    "\\hat{y} = \\sigma \\left( \\sum_{i=1}^{n} x_i w_i + b_i \\right)\n",
    "$$\n",
    "- Back Propagation -> Gradient Descent implemented on a network<br>\n",
    "*Loss function* help qualify he deviation from the expected output.\n",
    "    - use loss function\n",
    "    - go backwards & adjust initial weights and bias\n",
    "    - values adjust for better model\n",
    "\n",
    "### Learning Algorithm\n",
    "- Initialize Parameter with random values\n",
    "- Feed input data to network\n",
    "- Compare the predict value with expected value & calculate loss\n",
    "- Perform Back Propagation to propagate this loss back through the network\n",
    "- Update Parameters based on the loss\n",
    "- Iterate previous steps till loss is  *minimized*\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms used in neural network\n",
    "\n",
    "### Activation Functions\n",
    "- Non-linearity in the network\n",
    "- Decide whether a neuron can contribute to the next layer\n",
    "- Functions:\n",
    "    1. Step Function:<br>\n",
    "    if value>0(threshold) -> activate, else -> do not activate \n",
    "    2. Linear Function:<br>\n",
    "    Derivative is constant\n",
    "    3. Sigmoid Function **Binary Classification**\n",
    "        - Non-linear\n",
    "        - Analog Outputs: $\\text{sig}(t) \\in (0,1)$\n",
    "        - Vanishing Gradient Problem\n",
    "    4. Tanh Function\n",
    "        - Non-linear\n",
    "        - Derivative stepper than Sigmoid:  $\\text{tanh}(x) \\in (-1,1)$\n",
    "        - Vanishing Gradient Problem\n",
    "    5. ReLU Function (Rectified Linear Unit) **If unsure**\n",
    "        - Non-linear\n",
    "        - Sparse Activations\n",
    "        - $\\text{R}(z) \\in (0, \\infty)$\n",
    "        - Gradient = 0 -> Dying ReLU Problem\n",
    "    6. Leaky ReLU Function\n",
    "\n",
    "### Loss Functions\n",
    "*Regression:* Squared Error, Huber Loss\n",
    "\n",
    "*Binary Classification:* Binary Cross-Entropy, Hinge Loss\n",
    "\n",
    "*Multi-Class Classification:* Multi-Class Cross-Entropy, Kullback Divergence\n",
    "\n",
    "#### Optimizers\n",
    "Tie together the *loss function* and *model parameters* by **updating** network based on output of the loss function.\n",
    "\n",
    "*Loss Function* **guide** the optimizer.\n",
    "\n",
    "### Gradient Descent\n",
    "Iterative algorithm starts off at random point on the loss function and travels down its **slop** in steps until it reaches lowest point of the function.\n",
    "- Algorithm\n",
    "  1. calculate what small change in each individual weight would do to the loss function\n",
    "  2. adjust each parameter based on its gradient\n",
    "  3. repeat steps 1 and 2, until loss function is as low as possible\n",
    "- Learning Rate\n",
    "  - small number as 0.001\n",
    "  - ensure that any changes made to the weights are quite small\n",
    "- Stochastic Gradient Descent(SGD)\n",
    "  - use a **subset** of training examples rather than the *entire* lot\n",
    "  - implementation uses *batches* on each pass\n",
    "  - use *momentum* to accumulate gradients\n",
    "  - less intensive computationally\n",
    "\n",
    "### Parameters & Hyperparameter\n",
    "- Model parameters: can be estimated<br>\n",
    "Examples: weight, bias\n",
    "- Model hyperparameter: not estimated<br>\n",
    "configurations external to the neural network. Value can not be estimated right from the data<br>\n",
    "Example: Learning rate.\n",
    "\n",
    "### Epochs, Batches, Batch Sizes & Iterations\n",
    "Only if the dataset i large. **chunks** feed one-by-one\n",
    "- Epoches<br>\n",
    "Entire dataset is passed forward and backward through the neural network only **ONCE** <br>\n",
    "Multiple epochs to generalize better\n",
    "- Batches & Batch Size<br>\n",
    "Divide large dataset into smaller batches. Total number of training examples in a batch is the size.\n",
    "- Iteration<br>\n",
    "Number of batches needed to complete one epoch\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Learning\n",
    "\n",
    "### Supervised Learning\n",
    "Predict the correct label for unseen data.\n",
    "- Algorithm designed learn by example\n",
    "- Trained on *well-labeled* data\n",
    "- Examples consist of:\n",
    "  - Input object(vector)\n",
    "  - Desired output(supervisory signal)\n",
    "\n",
    "During training, SL algorithm searches for patterns that correlate with the desire output. After training, takes unseen inputs and determine which label to classify it to.\n",
    "1. Classification\n",
    "  - data into class/category\n",
    "  - models finds features in the data that correlate to a class and creates a *mapping function* -> classify unseen data\n",
    "  - Popular Algorithm\n",
    "    1. Linear Classifiers\n",
    "    2. Support Vector Machines\n",
    "    3. K-Nearest Neighbor\n",
    "    4. Random Forest\n",
    "2. Regression\n",
    "  - Find relationship between dependent & independent variables\n",
    "  - predict continues value\n",
    "  - Popular Algorithm\n",
    "    1. Linear Regression\n",
    "    2. Lasso Regression\n",
    "    3. Multivariate Regression\n",
    "\n",
    "- Applications\n",
    "  - Bioinformatics\n",
    "  - Object Recognition\n",
    "  - Spam Detection\n",
    "  - Speech Recognition\n",
    "\n",
    "### Unsupervised Learning\n",
    "- Uses to manifest **underlying patterns** in data\n",
    "- Used in exploratory data analysis\n",
    "- Does not use labelled data, rather relies on the **data features**\n",
    "- **Goal:** Analyze data and find important underlying patterns\n",
    "\n",
    "1. Clustering<br>\n",
    "    grouping data into different clusters or groups\n",
    "    - Partition Clustering -> single cluster\n",
    "    - Hierarchical Clustering -> clusters within clusters\n",
    "- Popular Algorithm\n",
    "  1. K-Means\n",
    "  2. Expectation Maximization\n",
    "  3. Hierachical Cluster Analysis(HCA)  \n",
    "2. Association<br>\n",
    "   find relationship between different entities\n",
    "\n",
    "- Application\n",
    "  - AirBnB\n",
    "  - Amazon\n",
    "  - Credit card fraud detection\n",
    "\n",
    "### Reinforcement Learning\n",
    "  - Enables an *agent* to learn in an interactive *environment* by trail & error based on feedback from its own actions&experience\n",
    "  - Use *rewards* & *punishments* as signals for positive & negative behavior\n",
    "  - Goal:model that gets maximum reward\n",
    "\n",
    "Model as *Markov Decision Process*\n",
    "- Application\n",
    "  - Robotics\n",
    "  - Business Strategy Planning\n",
    "  - Traffic Light Control\n",
    "  - Web System Configuration\n",
    "\n",
    "### Regularization\n",
    "Core Problem: both training data & NEW data, most common problem is *Overfitting*\n",
    "\n",
    "- Tackling Overfitting\n",
    "1. Dropout: \n",
    "randomly removes some nodes & their connections\n",
    "2. Dataset Augmentation:\n",
    "More data -> better model<br>\n",
    "apply transformation on existing dataset to get synthesize more data\n",
    "3. Early Stopping : training error decreases and validation error increases\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Architectures\n",
    "\n",
    "### Fully-Connected Feed Forward NN\n",
    "Each neuron is connected to every subsequent layer with no backward connections\n",
    "- Inputs\n",
    "- Output\n",
    "- Hidden Layers\n",
    "- Neurons per hidden layer\n",
    "- Activation functions\n",
    "\n",
    "### Recurrent NN\n",
    "- Feed-Forward Neural Networks<br>\n",
    "take fixed-sized input and return fixed-sized outputs\n",
    "- Vanilla NN can not process *sequence data*\n",
    "- *feedback loop* in the hidden layer\n",
    "- Train RNN\n",
    "  - back propagation algorithm\n",
    "  - applied for every *sequence data point*\n",
    "  - back propagation through time(BTT)\n",
    "  - short-term memory of a RNN is due to Vanishing Gradient Problem(VGP)\n",
    "\n",
    "### LSTMs & GRNNs\n",
    "- Long Short Term Memory\n",
    "  - Update Gate\n",
    "  - Reset Gate\n",
    "  - Forget Gate\n",
    "- Gated RNN\n",
    "  - Updated Gate\n",
    "  - Reset Gate\n",
    "- Application\n",
    "  - NLP\n",
    "  - Sentiment Analysis\n",
    "  - DNA sequence classification\n",
    "  - Speech recognition\n",
    "  - Language translation\n",
    "\n",
    "### Convolutional NNS (CNN)\n",
    "- inspired by the organization of neurons in the *visual cortex* of human brain\n",
    "- Good for processing like image, audio and video \n",
    "- Hidden Layers\n",
    "  - convolutional layers<br>\n",
    "    Input -> 2D, Output -> 1D, extract features in *chunks* by *kernel*\n",
    "  - pooling layers<br>\n",
    "    reduce the number of neurons: 1. Max & 2. Min pooling\n",
    "  - fully-connectd layers\n",
    "  - normalization layers\n",
    "- Application\n",
    "  - Computer vision\n",
    "  - image recognition\n",
    "  - image processing\n",
    "  - image segmentation\n",
    "  - video analysis\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DL Model\n",
    "\n",
    "1. Gathering Data<br>\n",
    " - picking data is the key\n",
    " - Size\n",
    "   - amount of data = 10x number of model parameters\n",
    "   - regression: 10 examples per predictor variable\n",
    "   - image: 1000 images per class\n",
    " - Quality\n",
    "   - label errors\n",
    "   - noisy\n",
    " - resource:\n",
    "   - [UCI](archive.ics.uci.edu)\n",
    "   - [kaggle](kaggle.com/datasets)\n",
    "   - [Google dataset](datasetsearch.research.google.com)\n",
    "2. Preprocessing the data\n",
    "- splitting dataset into *subsets*\n",
    "   - Train on *training data*\n",
    "   - Evaluate on *validation data*\n",
    "   - Test it on *testing data*\n",
    "   - hyperparameters -> decide how big the validation set\n",
    "   - Cross-Validation\n",
    "- Formatting -> CSV file\n",
    "- Missing data -> 'NaN' or 'Null'\n",
    "   - Eliminating\n",
    "   - imputing the missing value\n",
    "- Sampling -> small sample of the dataset<br>\n",
    "    Example weight = Original Weight x Downsampling Factor\n",
    "    - Faster convergence\n",
    "    - Reduce disk space\n",
    "    - dataset is in *similar ratio*\n",
    "- Feature Scaling\n",
    "  1. Normalization\n",
    "  2. Standardization  \n",
    "3. Training the model<br>\n",
    "Feed data -> Forward propagation -> Loss function -> Back propagation\n",
    "4. Evaluation<br>\n",
    "Test the model on *validation*\n",
    "5. Optimization\n",
    "    - Hyperparameter Tuning\n",
    "      - increase number of epochs\n",
    "      - adjust learning rate\n",
    "      - initial condition play large role\n",
    "    - Addressing Overfitting\n",
    "      - Getting more data/Reducing model size\n",
    "      - Regularization: L1 / L2\n",
    "    - Data Augmentation\n",
    "      - artificially increasing dataset \n",
    "    - Dropout\n",
    "      - randomly drops out some neurons\n",
    "      - reduce co-dependency if neurons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
