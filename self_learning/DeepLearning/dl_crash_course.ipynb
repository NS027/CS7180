{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Crash Course for Beginners\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is deep learning\n",
    "### Classification\n",
    "- **AI**\n",
    "- **Machine Learning:**\n",
    "Teaching computer to Recognize Patterns and Data\n",
    "- **Deep Learning:**<br>\n",
    "Technique that learns features and tasks directly from data.\n",
    "    - neural networks: hidden layers\n",
    "\n",
    "### why now\n",
    "- Data is prevalent\n",
    "- Improved hardware architectures\n",
    "- Tensorflow/Pytorch\n",
    "\n",
    "### Neural Networks\n",
    "- Input layer -> hidden layer -> output layer\n",
    "- neurons\n",
    "\n",
    "### Learning Process\n",
    "- Forward Propagation: *creation function*\n",
    "  - *Weight*: how important is the neuron\n",
    "  - *Bias*: allows for the shifting of the $\\sigma$ to the right or left\n",
    "$$\n",
    "\\hat{y} = \\sigma \\left( \\sum_{i=1}^{n} x_i w_i + b_i \\right)\n",
    "$$\n",
    "- Back Propagation -> Gradient Descent implemented on a network<br>\n",
    "*Loss function* help qualify he deviation from the expected output.\n",
    "    - use loss function\n",
    "    - go backwards & adjust initial weights and bias\n",
    "    - values adjust for better model\n",
    "\n",
    "### Learning Algorithm\n",
    "- Initialize Parameter with random values\n",
    "- Feed input data to network\n",
    "- Compare the predict value with expected value & calculate loss\n",
    "- Perform Back Propagation to propagate this loss back through the network\n",
    "- Update Parameters based on the loss\n",
    "- Iterate previous steps till loss is  *minimized*\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms used in neural network\n",
    "\n",
    "### Activation Functions\n",
    "- Non-linearity in the network\n",
    "- Decide whether a neuron can contribute to the next layer\n",
    "- Functions:\n",
    "    1. Step Function:<br>\n",
    "    if value>0(threshold) -> activate, else -> do not activate \n",
    "    2. Linear Function:<br>\n",
    "    Derivative is constant\n",
    "    3. Sigmoid Function **Binary Classification**\n",
    "        - Non-linear\n",
    "        - Analog Outputs: $\\text{sig}(t) \\in (0,1)$\n",
    "        - Vanishing Gradient Problem\n",
    "    4. Tanh Function\n",
    "        - Non-linear\n",
    "        - Derivative stepper than Sigmoid:  $\\text{tanh}(x) \\in (-1,1)$\n",
    "        - Vanishing Gradient Problem\n",
    "    5. ReLU Function (Rectified Linear Unit) **If unsure**\n",
    "        - Non-linear\n",
    "        - Sparse Activations\n",
    "        - $\\text{R}(z) \\in (0, \\infty)$\n",
    "        - Gradient = 0 -> Dying ReLU Problem\n",
    "    6. Leaky ReLU Function\n",
    "\n",
    "### Loss Functions\n",
    "<span style=\"color:green; font-weight:bold;\">Regression:</span> Squared Error, Huber Loss\n",
    "\n",
    "<span style=\"color:green; font-weight:bold;\">Binary Classification:</span> Binary Cross-Entropy, Hinge Loss\n",
    "\n",
    "<span style=\"color:green; font-weight:bold;\">Multi-Class Classification:</span> Multi-Class Cross-Entropy, Kullback Divergence\n",
    "\n",
    "#### Optimizers\n",
    "Tie together the *loss function* and *model parameters* by **updating** network based on output of the loss function.\n",
    "\n",
    "*Loss Function* **guide** the optimizer.\n",
    "\n",
    "### Gradient Descent\n",
    "Iterative algorithm starts off at random point on the loss function and travels down its **slop** in steps until it reaches lowest point of the function.\n",
    "- Algorithm\n",
    "  1. calculate what small change in each individual weight would do to the loss function\n",
    "  2. adjust each parameter based on its gradient\n",
    "  3. repeat steps 1 and 2, until loss function is as low as possible\n",
    "- Learning Rate\n",
    "  - small number as 0.001\n",
    "  - ensure that any changes made to the weights are quite small\n",
    "- Stochastic Gradient Descent(SGD)\n",
    "  - use a **subset** of training examples rather than the *entire* lot\n",
    "  - implementation uses *batches* on each pass\n",
    "  - use *momentum* to accumulate gradients\n",
    "  - less intensive computationally\n",
    "\n",
    "### Parameters & Hyperparameter\n",
    "- Model parameters: can be estimated<br>\n",
    "Examples: weight, bias\n",
    "- Model hyperparameter: not estimated<br>\n",
    "configurations external to the neural network. Value can not be estimated right from the data<br>\n",
    "Example: Learning rate.\n",
    "\n",
    "### Epochs, Batches, Batch Sizes & Iterations\n",
    "Only if the dataset i large. **chunks** feed one-by-one\n",
    "- Epoches<br>\n",
    "Entire dataset is passed forward and backward through the neural network only **ONCE** <br>\n",
    "Multiple epochs to generalize better\n",
    "- Batches & Batch Size<br>\n",
    "Divide large dataset into smaller batches. Total number of training examples in a batch is the size.\n",
    "- Iteration<br>\n",
    "Number of batches needed to complete one epoch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
